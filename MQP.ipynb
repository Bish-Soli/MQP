{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bish-Soli/MQP/blob/main/MQP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR 10-LT AND CIFAR 100-LT"
      ],
      "metadata": {
        "id": "wymmQcD4KkWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset"
      ],
      "metadata": {
        "id": "5ZRXZvuG_3EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = load_dataset('cifar100')"
      ],
      "metadata": {
        "id": "u8dqtUXB_48f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np"
      ],
      "metadata": {
        "id": "IRk2zxmEBNHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import transforms"
      ],
      "metadata": {
        "id": "HH823baZBdRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader = DataLoader(dataset)"
      ],
      "metadata": {
        "id": "6Rv2zCjRBgUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# batch_size = 4\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=2)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# classes = ('plane', 'car', 'bird', 'cat',\n",
        "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "DVDHojSSB7u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "V1hqVnI4RTN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from __future__ import print_function\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "To8fkMmnRRQJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard_logger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmd8m_V2STS4",
        "outputId": "a77f7aaf-74e8-446a-e99d-fdbb1ec700e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard_logger in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.11.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class ImbalanceCIFAR(Dataset):\n",
        "    def __init__(self, cifar_version=10, root='./data', train=True, transform=None, imbalance_ratio=0.1):\n",
        "        super(ImbalanceCIFAR, self).__init__()\n",
        "        if cifar_version == 10:\n",
        "            self.original_dataset = torchvision.datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
        "        elif cifar_version == 100:\n",
        "            self.original_dataset = torchvision.datasets.CIFAR100(root=root, train=train, download=True, transform=transform)\n",
        "        else:\n",
        "            raise ValueError(\"CIFAR version must be 10 or 100\")\n",
        "\n",
        "        self.num_classes = 10 if cifar_version == 10 else 100\n",
        "        self._create_long_tailed(imbalance_ratio)\n",
        "\n",
        "    def _create_long_tailed(self, imbalance_ratio):\n",
        "        # Get class distribution\n",
        "        class_counts = np.bincount([label for _, label in self.original_dataset])\n",
        "        # Compute number of samples for least represented class\n",
        "        num_samples_lt = [int(count * imbalance_ratio) for count in class_counts]\n",
        "        self.indices = []\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = np.where(np.array(self.original_dataset.targets) == i)[0]\n",
        "            np.random.shuffle(class_indices)\n",
        "            selected_indices = class_indices[:num_samples_lt[i]]\n",
        "            self.indices.extend(selected_indices)\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        return self.original_dataset[real_idx]\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        class_counts = np.bincount([self.original_dataset.targets[idx] for idx in self.indices])\n",
        "        return {i: class_counts[i] for i in range(self.num_classes)}\n",
        "\n",
        "    def plot_class_distribution(self):\n",
        "        distribution = self.get_class_distribution()\n",
        "        plt.bar(distribution.keys(), distribution.values())\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Number of samples')\n",
        "        plt.title('Class Distribution in Dataset')\n",
        "        plt.show()\n",
        "\n",
        "    def get_samples_from_each_class(self, num_samples=1):\n",
        "        samples = {}\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = [idx for idx in self.indices if self.original_dataset.targets[idx] == i]\n",
        "            np.random.shuffle(class_indices)\n",
        "            samples[i] = [self.original_dataset[class_indices[j]] for j in range(num_samples)]\n",
        "        return samples\n",
        "\n",
        "    def imshow(img):\n",
        "        img = img.numpy().transpose((1, 2, 0))  # Convert from tensor image\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "        img = std * img + mean  # Unnormalize\n",
        "        img = np.clip(img, 0, 1)  # Clip to [0, 1]\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    def show_augmented_images(self, image_index, augmentations, num_samples=5):\n",
        "        original_image, _ = self.original_dataset[image_index]\n",
        "        images = [augmentations(original_image) for _ in range(num_samples)]\n",
        "        grid_image = torchvision.utils.make_grid(images, nrow=num_samples)\n",
        "        self.imshow(grid_image)\n",
        "\n",
        "\n",
        "    def compute_imbalance_ratio(self):\n",
        "        class_distribution = self.get_class_distribution()\n",
        "        max_count = max(class_distribution.values())\n",
        "        min_count = min(class_distribution.values())\n",
        "        return max_count / min_count\n",
        "\n",
        "    def get_random_batch(self, batch_size=32):\n",
        "        indices = np.random.choice(self.indices, batch_size, replace=False)\n",
        "        return [self.original_dataset[idx] for idx in indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImbalanceCIFAR10(ImbalanceCIFAR):\n",
        "    def __init__(self, root='./data', train=True, transform=None, imbalance_ratio=0.1):\n",
        "        super(ImbalanceCIFAR10, self).__init__(cifar_version=10, root=root, train=train, transform=transform, imbalance_ratio=imbalance_ratio)\n",
        "\n",
        "\n",
        "class ImbalanceCIFAR100(ImbalanceCIFAR):\n",
        "    def __init__(self, root='./data', train=True, transform=None, imbalance_ratio=0.1):\n",
        "        super(ImbalanceCIFAR100, self).__init__(cifar_version=100, root=root, train=train, transform=transform, imbalance_ratio=imbalance_ratio)\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Initialize the datasets\n",
        "cifar10_lt = ImbalanceCIFAR10(train=True, transform=transform, imbalance_ratio=0.1)\n",
        "cifar100_lt = ImbalanceCIFAR100(train=True, transform=transform, imbalance_ratio=0.02)\n",
        "cifar10_test = ImbalanceCIFAR10(train=False, transform=test_transform, imbalance_ratio=1)  # imbalance_ratio=1 to keep original distribution\n",
        "cifar100_test = ImbalanceCIFAR100(train=False, transform=test_transform, imbalance_ratio=1)  # imbalance_ratio=1 to keep original distribution\n",
        "\n",
        "# Define the size of the validation set\n",
        "val_size_cifar10 = int(0.2 * len(cifar10_lt))  # e.g., 20% of the training data\n",
        "train_size_cifar10 = len(cifar10_lt) - val_size_cifar10\n",
        "\n",
        "val_size_cifar100 = int(0.2 * len(cifar100_lt))  # e.g., 20% of the training data\n",
        "train_size_cifar100 = len(cifar100_lt) - val_size_cifar100\n",
        "\n",
        "# Split the datasets\n",
        "cifar10_lt_train, cifar10_lt_val = random_split(cifar10_lt, [train_size_cifar10, val_size_cifar10])\n",
        "cifar100_lt_train, cifar100_lt_val = random_split(cifar100_lt, [train_size_cifar100, val_size_cifar100])\n",
        "\n",
        "# Initialize data loaders for training\n",
        "train_loader_cifar10_lt = DataLoader(cifar10_lt_train, batch_size=4, shuffle=True)\n",
        "train_loader_cifar100_lt = DataLoader(cifar100_lt_train, batch_size=4, shuffle=True)\n",
        "\n",
        "# Create validation DataLoaders\n",
        "val_loader_cifar10_lt = DataLoader(cifar10_lt_val, batch_size=4, shuffle=False)\n",
        "val_loader_cifar100_lt = DataLoader(cifar100_lt_val, batch_size=4, shuffle=False)\n",
        "\n",
        "# Create testing DataLoaders\n",
        "test_loader_cifar10_lt = DataLoader(cifar10_test, batch_size=4, shuffle=False)\n",
        "test_loader_cifar100_lt = DataLoader(cifar100_test, batch_size=4, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOuWt_37VzOW",
        "outputId": "59a21ac1-9e7e-4870-bdb3-fe4e030be2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48980725.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 49011766.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, weight_normalization = False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.weight_normalization = weight_normalization\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.weight_normalization == True:\n",
        "           self.max_norm_(self.conv.weight)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x= self.pool(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def max_norm_(self, w, _max_norm_val = 3):\n",
        "      with torch.no_grad():\n",
        "        norm = w.norm(2, dim=0, keepdim=True).clamp(min=_max_norm_val / 2)\n",
        "        desired = torch.clamp(norm, max=_max_norm_val)\n",
        "        w *= (desired / norm)\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, weight_normalization = False):\n",
        "        super().__init__()\n",
        "        self.backbone = # Load in a pre-trained Resnet, remove the last layer (fine tune the resnet)\n",
        "        # resnet -> 2048\n",
        "        # self.conv1 = ConvBlock(3, 6, 5, weight_normalization=weight_normalization)\n",
        "        # self.conv2 = ConvBlock(6, 16, 5, weight_normalization=weight_normalization)\n",
        "        self.fc1 = nn.Linear(2048, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iNqyQRIICWX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassSetup:\n",
        "\n",
        "  def __init__(self,lr = 0.001, weight_decay=0, weight_normalization= False,opt='Adam', loss_type = 'metric'):\n",
        "    self.weight_decay = weight_decay\n",
        "    self.weight_normalization = weight_normalization\n",
        "    self.lr = lr\n",
        "    self.model = Net(weight_normalization)\n",
        "    self.opt = self.get_optimizer(opt)\n",
        "    self.loss = self.get_loss(loss_type)\n",
        "\n",
        "\n",
        "  def get_optimizer(self, opt):\n",
        "    if opt == 'Adam':\n",
        "      return torch.optim.Adam(self.model.parameters(), lr = self.lr, weight_decay = self.weight_decay)\n",
        "    else:\n",
        "     return torch.optim.SGD(self.model.parameters(), lr = self.lr, weight_decay = self.weight_decay)\n",
        "\n",
        "  def get_loss(self, loss_type):\n",
        "    if loss_type =='metric':\n",
        "      return nn.CrossEntropyLoss()\n",
        "    # elif self.loss_type == 'contrastive':\n",
        "    #   self.loss = ContrastiveLoss()\n",
        "    # else\n",
        "    #   self.loss = Hybrid()\n",
        "\n",
        "  def get_all (self):\n",
        "    return self.opt, self.loss, self.model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUwGYpoXQQ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SupConLoss(nn.Module):\n",
        "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
        "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
        "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
        "                 base_temperature=0.07):\n",
        "        super(SupConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "\n",
        "    def forward(self, features, labels=None, mask=None):\n",
        "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
        "        it degenerates to SimCLR unsupervised loss:\n",
        "        https://arxiv.org/pdf/2002.05709.pdf\n",
        "\n",
        "        Args:\n",
        "            features: hidden vector of shape [bsz, n_views, ...].\n",
        "            labels: ground truth of shape [bsz].\n",
        "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                has the same class as sample i. Can be asymmetric.\n",
        "        Returns:\n",
        "            A loss scalar.\n",
        "        \"\"\"\n",
        "        device = (torch.device('cuda')\n",
        "                  if features.is_cuda\n",
        "                  else torch.device('cpu'))\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float().to(device)\n",
        "        else:\n",
        "            mask = mask.float().to(device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        # compute logits\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T),\n",
        "            self.temperature)\n",
        "        # for numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # tile mask\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "        # mask-out self-contrast cases\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        # compute log_prob\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # compute mean of log-likelihood over positive\n",
        "        # modified to handle edge cases when there is no positive pair\n",
        "        # for an anchor point.\n",
        "        # Edge case e.g.:-\n",
        "        # features of shape: [4,1,...]\n",
        "        # labels:            [0,1,1,2]\n",
        "        # loss before mean:  [nan, ..., ..., nan]\n",
        "        mask_pos_pairs = mask.sum(1)\n",
        "        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
        "\n",
        "        # loss\n",
        "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "dZ1Bjg6MNIna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "setup = ClassSetup(weight_normalization=True, weight_decay=1e-4)\n",
        "optimizer, criterion, net = setup.get_all()\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YdEgCliCe5v",
        "outputId": "9d1d8a3d-b055-4dcf-fab7-3772492af521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): ConvBlock(\n",
              "    (conv): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): ConvBlock(\n",
              "    (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "        elif val_loss > self.best_score - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "ic7H2n71eGUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "supcon_loss = SupConLoss(temperature=0.07, contrast_mode='all', base_temperature=0.07)\n",
        "print_every_batches = 1000  # subject to change from 2000->1000 to print down there\n",
        "\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    net.train()  # Set the model to training mode\n",
        "\n",
        "    for i, data in enumerate(train_loader_cifar10_lt, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        #inputs.to(device)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "        #     running_loss = 0.0\n",
        "        if i % print_every_batches == print_every_batches - 1:    # Adjust this condition\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_every_batches:.3f}')\n",
        "            #running_loss = 0.0\n",
        "    # print(f'End of Epoch {epoch + 1}, loss: {running_loss / len(train_loader_cifar10_lt):.3f}')\n",
        "    training_loss = running_loss / len(train_loader_cifar10_lt)\n",
        "    # print(f'End of Epoch {epoch + 1}, Training Loss: {training_loss:.3f}')\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = 0.0\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_labels in val_loader_cifar10_lt:\n",
        "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "            val_outputs = net(val_inputs)\n",
        "            v_loss = criterion(val_outputs, val_labels)\n",
        "            val_loss += v_loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader_cifar10_lt)\n",
        "    print(f'End of Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader_cifar10_lt):.3f}, Validation Loss: {val_loss:.3f}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Early stopping check\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9-5qqipCgn0",
        "outputId": "bdad4544-5147-45e0-c7b7-76d91f57a8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  1000] loss: 2.006\n",
            "End of Epoch 1, Training Loss: 2.006, Validation Loss: 1.775\n",
            "[2,  1000] loss: 1.792\n",
            "End of Epoch 2, Training Loss: 1.792, Validation Loss: 1.686\n",
            "[3,  1000] loss: 1.646\n",
            "End of Epoch 3, Training Loss: 1.646, Validation Loss: 1.631\n",
            "[4,  1000] loss: 1.535\n",
            "End of Epoch 4, Training Loss: 1.535, Validation Loss: 1.576\n",
            "[5,  1000] loss: 1.450\n",
            "End of Epoch 5, Training Loss: 1.450, Validation Loss: 1.529\n",
            "[6,  1000] loss: 1.358\n",
            "End of Epoch 6, Training Loss: 1.358, Validation Loss: 1.533\n",
            "[7,  1000] loss: 1.291\n",
            "End of Epoch 7, Training Loss: 1.291, Validation Loss: 1.532\n",
            "[8,  1000] loss: 1.198\n",
            "End of Epoch 8, Training Loss: 1.198, Validation Loss: 1.603\n",
            "[9,  1000] loss: 1.107\n",
            "End of Epoch 9, Training Loss: 1.107, Validation Loss: 1.611\n",
            "[10,  1000] loss: 1.050\n",
            "End of Epoch 10, Training Loss: 1.050, Validation Loss: 1.691\n",
            "Early stopping triggered\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()  # Set the model to evaluation mode\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader_cifar10_lt:  # using CIFAR10 test loader as an example\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni7tkauQY_n1",
        "outputId": "21b29cb0-1e79-4901-c00e-e5f599662ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 47 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Learning\n",
        "https://github.com/HobbitLong/SupContrast/tree/master"
      ],
      "metadata": {
        "id": "XRoy5NW4O0OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##resnet_big.py"
      ],
      "metadata": {
        "id": "31BGH0OrPS6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ResNet in PyTorch.\n",
        "ImageNet-Style ResNet\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "Adapted from: https://github.com/bearpaw/pytorch-classification\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves\n",
        "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
        "        # https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = strides[i]\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, layer=100):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "\n",
        "\n",
        "model_dict = {\n",
        "    'resnet18': [resnet18, 512],\n",
        "    'resnet34': [resnet34, 512],\n",
        "    'resnet50': [resnet50, 2048],\n",
        "    'resnet101': [resnet101, 2048],\n",
        "}\n",
        "\n",
        "\n",
        "class LinearBatchNorm(nn.Module):\n",
        "    \"\"\"Implements BatchNorm1d by BatchNorm2d, for SyncBN purpose\"\"\"\n",
        "    def __init__(self, dim, affine=True):\n",
        "        super(LinearBatchNorm, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.bn = nn.BatchNorm2d(dim, affine=affine)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.dim, 1, 1)\n",
        "        x = self.bn(x)\n",
        "        x = x.view(-1, self.dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SupConResNet(nn.Module):\n",
        "    \"\"\"backbone + projection head\"\"\"\n",
        "    def __init__(self, name='resnet50', head='mlp', feat_dim=128, num_classes = 10):\n",
        "        super(SupConResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        self.num_classes = num_classes\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "        if head == 'linear':\n",
        "            self.head = nn.Linear(dim_in, feat_dim)\n",
        "        elif head == 'mlp':\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(dim_in, dim_in),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(dim_in, feat_dim)\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                'head not supported: {}'.format(head))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feat = self.encoder(x)\n",
        "        feat = F.normalize(self.head(feat), dim=1)\n",
        "        bsz = int(feat.shape[0]/2)\n",
        "        f1, f2 = torch.split(feat, [bsz, bsz], dim=0)\n",
        "        x = self.fc(f1)\n",
        "        return feat, x\n",
        "\n",
        "\n",
        "class SupCEResNet(nn.Module):\n",
        "    \"\"\"encoder + classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(SupCEResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        self.fc = nn.Linear(dim_in, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.encoder(x))\n",
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"Linear classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        _, feat_dim = model_dict[name]\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)"
      ],
      "metadata": {
        "id": "95IVltB_PQVk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Losses.py"
      ],
      "metadata": {
        "id": "CDbI9_E5QDBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Yonglong Tian (yonglong@mit.edu)\n",
        "Date: May 07, 2020\n",
        "\"\"\"\n",
        "\n",
        "class SupConLoss(nn.Module):\n",
        "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
        "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
        "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
        "                 base_temperature=0.07):\n",
        "        super(SupConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "\n",
        "    def forward(self, features, labels=None, mask=None):\n",
        "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
        "        it degenerates to SimCLR unsupervised loss:\n",
        "        https://arxiv.org/pdf/2002.05709.pdf\n",
        "\n",
        "        Args:\n",
        "            features: hidden vector of shape [bsz, n_views, ...].\n",
        "            labels: ground truth of shape [bsz].\n",
        "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                has the same class as sample i. Can be asymmetric.\n",
        "        Returns:\n",
        "            A loss scalar.\n",
        "        \"\"\"\n",
        "        device = (torch.device('cuda')\n",
        "                  if features.is_cuda\n",
        "                  else torch.device('cpu'))\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float().to(device)\n",
        "        else:\n",
        "            mask = mask.float().to(device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        # compute logits\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T),\n",
        "            self.temperature)\n",
        "        # for numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # tile mask\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "        # mask-out self-contrast cases\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        # compute log_prob\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # compute mean of log-likelihood over positive\n",
        "        # modified to handle edge cases when there is no positive pair\n",
        "        # for an anchor point.\n",
        "        # Edge case e.g.:-\n",
        "        # features of shape: [4,1,...]\n",
        "        # labels:            [0,1,1,2]\n",
        "        # loss before mean:  [nan, ..., ..., nan]\n",
        "        mask_pos_pairs = mask.sum(1)\n",
        "        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
        "\n",
        "        # loss\n",
        "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "9vxrmuAXQE2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Util.py"
      ],
      "metadata": {
        "id": "qwZSOVS3PdRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoCropTransform:\n",
        "    \"\"\"Create two crops of the same image\"\"\"\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.transform(x), self.transform(x)]\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, epoch):\n",
        "    lr = args.learning_rate\n",
        "    if args.cosine:\n",
        "        eta_min = lr * (args.lr_decay_rate ** 3)\n",
        "        lr = eta_min + (lr - eta_min) * (\n",
        "                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
        "    else:\n",
        "        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
        "        if steps > 0:\n",
        "            lr = lr * (args.lr_decay_rate ** steps)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n",
        "    if args.warm and epoch <= args.warm_epochs:\n",
        "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
        "            (args.warm_epochs * total_batches)\n",
        "        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def set_optimizer(opt, model):\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=opt.learning_rate,\n",
        "                          momentum=opt.momentum,\n",
        "                          weight_decay=opt.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, opt, epoch, save_file):\n",
        "    print('==> Saving...')\n",
        "    state = {\n",
        "        'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(state, save_file)\n",
        "    del state"
      ],
      "metadata": {
        "id": "aMJ8kk1yPLEp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main_supcon.py"
      ],
      "metadata": {
        "id": "c9rG-nLIQZ4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "\n",
        "import tensorboard_logger as tb_logger\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import math\n",
        "\n",
        "def parse_option(batch_size=32, epochs=10, print_freq=2, save_freq=9, num_workers=16,\n",
        "                 learning_rate=0.05, lr_decay_epochs='700,800,900', lr_decay_rate=0.1,\n",
        "                 weight_decay=1e-4, momentum=0.9, model='resnet50', dataset='cifar10',\n",
        "                 mean=None, std=None, data_folder=None, size=32, method='SupCon',\n",
        "                 temp=0.07, cosine=False, syncBN=False, warm=False, trial='0'):\n",
        "    class Option(object):\n",
        "        pass\n",
        "\n",
        "    opt = Option()\n",
        "\n",
        "    # Set values\n",
        "    opt.batch_size = batch_size\n",
        "    opt.epochs = epochs\n",
        "    opt.print_freq = print_freq\n",
        "    opt.save_freq = save_freq\n",
        "    opt.num_workers = num_workers\n",
        "    opt.learning_rate = learning_rate\n",
        "    opt.lr_decay_epochs = lr_decay_epochs\n",
        "    opt.lr_decay_rate = lr_decay_rate\n",
        "    opt.weight_decay = weight_decay\n",
        "    opt.momentum = momentum\n",
        "    opt.model = model\n",
        "    opt.dataset = dataset\n",
        "    opt.mean = mean\n",
        "    opt.std = std\n",
        "    opt.data_folder = data_folder\n",
        "    opt.size = size\n",
        "    opt.method = method\n",
        "    opt.temp = temp\n",
        "    opt.cosine = cosine\n",
        "    opt.syncBN = syncBN\n",
        "    opt.warm = warm\n",
        "    opt.trial = trial\n",
        "\n",
        "    # Additional processing as in the original function\n",
        "    if opt.dataset == 'path':\n",
        "        assert opt.data_folder is not None and opt.mean is not None and opt.std is not None\n",
        "\n",
        "    if opt.data_folder is None:\n",
        "        opt.data_folder = './datasets/'\n",
        "\n",
        "    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n",
        "    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n",
        "\n",
        "    iterations = opt.lr_decay_epochs.split(',')\n",
        "    opt.lr_decay_epochs = [int(it) for it in iterations]\n",
        "\n",
        "    opt.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.format(\n",
        "        opt.method, opt.dataset, opt.model, opt.learning_rate,\n",
        "        opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n",
        "\n",
        "    if opt.cosine:\n",
        "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
        "\n",
        "    if opt.batch_size > 256:\n",
        "        opt.warm = True\n",
        "    if opt.warm:\n",
        "        opt.model_name = '{}_warm'.format(opt.model_name)\n",
        "        opt.warmup_from = 0.01\n",
        "        opt.warm_epochs = 10\n",
        "        if opt.cosine:\n",
        "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
        "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * \\\n",
        "                (1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
        "        else:\n",
        "            opt.warmup_to = opt.learning_rate\n",
        "\n",
        "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
        "    if not os.path.isdir(opt.tb_folder):\n",
        "        os.makedirs(opt.tb_folder)\n",
        "\n",
        "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
        "    if not os.path.isdir(opt.save_folder):\n",
        "        os.makedirs(opt.save_folder)\n",
        "\n",
        "    return opt\n",
        "\n",
        "\n",
        "def set_loader(opt):\n",
        "    # construct data loader\n",
        "    if opt.dataset == 'cifar10':\n",
        "        mean = (0.4914, 0.4822, 0.4465)\n",
        "        std = (0.2023, 0.1994, 0.2010)\n",
        "    elif opt.dataset == 'cifar100':\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "    elif opt.dataset == 'path':\n",
        "        mean = eval(opt.mean)\n",
        "        std = eval(opt.std)\n",
        "    else:\n",
        "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    if opt.dataset == 'cifar10':\n",
        "        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
        "                                         transform=TwoCropTransform(train_transform),\n",
        "                                         download=True)\n",
        "    elif opt.dataset == 'cifar100':\n",
        "        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
        "                                          transform=TwoCropTransform(train_transform),\n",
        "                                          download=True)\n",
        "    elif opt.dataset == 'path':\n",
        "        train_dataset = datasets.ImageFolder(root=opt.data_folder,\n",
        "                                            transform=TwoCropTransform(train_transform))\n",
        "    else:\n",
        "        raise ValueError(opt.dataset)\n",
        "\n",
        "    train_sampler = None\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n",
        "        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def set_model(opt):\n",
        "    model = SupConResNet(name=opt.model)\n",
        "    criterion = SupConLoss(temperature=opt.temp)\n",
        "\n",
        "    # enable synchronized Batch Normalization\n",
        "    if opt.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, opt, ALPHA=0.8, BETA=0.2):\n",
        "    \"\"\"one epoch training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    CE = nn.CrossEntropyLoss()\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    for idx, (images, labels) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images = torch.cat([images[0], images[1]], dim=0)\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "\n",
        "        # warm-up learning rate\n",
        "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
        "\n",
        "\n",
        "        # compute loss\n",
        "        features, x = model(images)\n",
        "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
        "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
        "        if opt.method == 'SupCon':\n",
        "            SC_Loss = criterion(features, labels)\n",
        "            CE_Loss = CE(x, labels)\n",
        "            loss = ALPHA * SC_Loss + BETA * CE_Loss\n",
        "        elif opt.method == 'SimCLR':\n",
        "            loss = criterion(features)\n",
        "        else:\n",
        "            raise ValueError('contrastive method not supported: {}'.\n",
        "                             format(opt.method))\n",
        "\n",
        "        # update metric\n",
        "        losses.update(loss.item(), bsz)\n",
        "\n",
        "        # SGD\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print info\n",
        "        if (idx + 1) % opt.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def main():\n",
        "    opt = parse_option()\n",
        "\n",
        "    # build data loader\n",
        "    train_loader = set_loader(opt)\n",
        "\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model(opt)\n",
        "\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(opt, model)\n",
        "\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
        "\n",
        "    # training routine\n",
        "    for epoch in range(1, opt.epochs + 1):\n",
        "        adjust_learning_rate(opt, optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        time1 = time.time()\n",
        "        loss = train(train_loader, model, criterion, optimizer, epoch, opt)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # tensorboard logger\n",
        "        logger.log_value('loss', loss, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "        if epoch % opt.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, opt, epoch, save_file)\n",
        "\n",
        "    # save the last model\n",
        "    save_file = os.path.join(\n",
        "        opt.save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, opt, opt.epochs, save_file)\n"
      ],
      "metadata": {
        "id": "kXL_YgS0O3kq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_options(opt):\n",
        "    for attr in dir(opt):\n",
        "        # Filter out built-in attributes and methods\n",
        "        if not attr.startswith(\"__\"):\n",
        "            print(f'{attr}: {getattr(opt, attr)}')\n",
        "\n",
        "# Call this function to print the options\n",
        "print_options(opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "6ljVLbR7TTNJ",
        "outputId": "14fa1415-1455-4236-ed87-e83cb314ad29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8dcf7697c633>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Call this function to print the options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ojaHCAOBVQO1",
        "outputId": "f1a4fd61-813f-4f36-ce11-dfc9954bc988"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][2/1563]\tBT 0.169 (3.651)\tDT 0.000 (0.751)\tloss 3.771 (3.807)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][4/1563]\tBT 0.163 (1.887)\tDT 0.007 (0.377)\tloss 3.762 (3.798)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][6/1563]\tBT 0.171 (1.315)\tDT 0.000 (0.252)\tloss 3.762 (3.788)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][8/1563]\tBT 0.174 (1.030)\tDT 0.001 (0.190)\tloss 3.779 (3.785)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][10/1563]\tBT 0.170 (0.858)\tDT 0.000 (0.152)\tloss 3.779 (3.783)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][12/1563]\tBT 0.170 (0.744)\tDT 0.000 (0.127)\tloss 3.774 (3.782)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][14/1563]\tBT 0.172 (0.662)\tDT 0.000 (0.109)\tloss 3.774 (3.781)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][16/1563]\tBT 0.168 (0.601)\tDT 0.000 (0.096)\tloss 3.774 (3.780)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][18/1563]\tBT 0.170 (0.553)\tDT 0.000 (0.085)\tloss 3.779 (3.780)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][20/1563]\tBT 0.168 (0.515)\tDT 0.000 (0.077)\tloss 3.773 (3.779)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][22/1563]\tBT 0.161 (0.484)\tDT 0.005 (0.071)\tloss 3.777 (3.779)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][24/1563]\tBT 0.189 (0.459)\tDT 0.000 (0.065)\tloss 3.776 (3.778)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][26/1563]\tBT 0.170 (0.437)\tDT 0.000 (0.060)\tloss 3.776 (3.778)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][28/1563]\tBT 0.166 (0.418)\tDT 0.000 (0.056)\tloss 3.773 (3.778)\n",
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 3, 32, 32])\n",
            "Train: [1][30/1563]\tBT 0.163 (0.402)\tDT 0.000 (0.053)\tloss 3.775 (3.778)\n",
            "torch.Size([64, 3, 32, 32])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-bc2796cf5d33>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, total time {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-bc2796cf5d33>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, opt, ALPHA, BETA)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b7d1d926ca65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b7d1d926ca65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b7d1d926ca65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mpreact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhQ4P-sVVg2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}