{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRhvMUhFCSRXS2Mhuz+hRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bish-Soli/MQP-LTR/blob/main/MQP_Losses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset,random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from __future__ import print_function\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "from PIL import Image\n",
        "import tarfile\n",
        "import math"
      ],
      "metadata": {
        "id": "bX733RfLm9pD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard_logger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRbieeAam_ic",
        "outputId": "eb8847ed-dbf8-4fa5-cf45-921f845bb79d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard_logger in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.11.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class ImbalanceCIFAR(Dataset):\n",
        "    def __init__(self, cifar_version=10, root='./data', train=True, transform=None, imbalance_ratio=0.01):\n",
        "        super(ImbalanceCIFAR, self).__init__()\n",
        "        if cifar_version == 10:\n",
        "            self.original_dataset = torchvision.datasets.CIFAR10(root=root, train=train, download=True, transform=transform)\n",
        "        elif cifar_version == 100:\n",
        "            self.original_dataset = torchvision.datasets.CIFAR100(root=root, train=train, download=True, transform=transform)\n",
        "        else:\n",
        "            raise ValueError(\"CIFAR version must be 10 or 100\")\n",
        "\n",
        "        self.num_classes = 10 if cifar_version == 10 else 100\n",
        "        self._create_long_tailed(imbalance_ratio)\n",
        "\n",
        "    def _create_long_tailed(self, imbalance_ratio):\n",
        "        # Get class distribution\n",
        "        class_counts = np.bincount([label for _, label in self.original_dataset])\n",
        "        # Compute number of samples for least represented class\n",
        "        #num_samples_lt = [int(count * imbalance_ratio) for count in class_counts]\n",
        "        # Compute number of samples for each class with exponential decrease\n",
        "        max_count = max(class_counts)\n",
        "\n",
        "        num_samples_lt = [int(max_count * (imbalance_ratio ** (i / (self.num_classes - 1.0)))) for i in range(self.num_classes)]\n",
        "        self.indices = []\n",
        "        self.targets = []\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = np.where(np.array(self.original_dataset.targets) == i)[0]\n",
        "            np.random.shuffle(class_indices)\n",
        "            selected_indices = class_indices[:num_samples_lt[i]]\n",
        "            self.indices.extend(selected_indices)\n",
        "            self.targets.extend([i] * len(selected_indices))\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        return self.original_dataset[real_idx]\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        class_counts = np.bincount([self.original_dataset.targets[idx] for idx in self.indices])\n",
        "        return {i: class_counts[i] for i in range(self.num_classes)}\n",
        "\n",
        "    def get_class_names(self):\n",
        "        return self.original_dataset.classes\n",
        "\n",
        "    def plot_class_distribution(self):\n",
        "        # distribution = self.get_class_distribution()\n",
        "        # minority_class = min(distribution, key=distribution.get)  # Get the class with the minimum number of samples\n",
        "        # bars = plt.bar(distribution.keys(), distribution.values())\n",
        "        # # Highlight the minority class with a different color\n",
        "        # bars[minority_class].set_color('y')  # Set to red or any color of your choice\n",
        "        # plt.bar(distribution.keys(), distribution.values())\n",
        "        # plt.xlabel('Class')\n",
        "        # plt.ylabel('Number of samples')\n",
        "        # plt.title('Class Distribution in Dataset')\n",
        "        # plt.show()\n",
        "        distribution = self.get_class_distribution()\n",
        "        # Sort classes by the number of samples per class\n",
        "        sorted_classes = sorted(distribution.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        # Separate the class indices and their corresponding counts\n",
        "        sorted_indices, sorted_counts = zip(*sorted_classes)\n",
        "\n",
        "        # Determine the threshold for minority classes, for example, you might define it as the lower 20%\n",
        "        threshold = np.percentile(sorted_counts, 30)\n",
        "\n",
        "        # Create a line plot for class distribution\n",
        "        plt.plot(sorted_indices, sorted_counts, label='Class Distribution')\n",
        "\n",
        "        # Fill the area under the curve\n",
        "        plt.fill_between(sorted_indices, sorted_counts, where=(np.array(sorted_counts) <= threshold), color='red', alpha=0.5, label='Minority classes')\n",
        "        plt.fill_between(sorted_indices, sorted_counts, where=(np.array(sorted_counts) > threshold), color='green', alpha=0.5, label='Majority classes')\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Sorted class indices (Large â†’ Small)')\n",
        "        plt.ylabel('Training samples per class')\n",
        "        plt.title('Class Distribution in Dataset')\n",
        "        plt.legend()\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "    def get_samples_from_each_class(self, num_samples=1):\n",
        "        samples = {}\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = [idx for idx in self.indices if self.original_dataset.targets[idx] == i]\n",
        "            np.random.shuffle(class_indices)\n",
        "            samples[i] = [self.original_dataset[class_indices[j]] for j in range(num_samples)]\n",
        "        return samples\n",
        "\n",
        "    def imshow(img):\n",
        "        img = img.numpy().transpose((1, 2, 0))  # Convert from tensor image\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "        img = std * img + mean  # Unnormalize\n",
        "        img = np.clip(img, 0, 1)  # Clip to [0, 1]\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    def show_augmented_images(self, image_index, augmentations, num_samples=5):\n",
        "        original_image, _ = self.original_dataset[image_index]\n",
        "        images = [augmentations(original_image) for _ in range(num_samples)]\n",
        "        grid_image = torchvision.utils.make_grid(images, nrow=num_samples)\n",
        "        self.imshow(grid_image)\n",
        "\n",
        "\n",
        "    def compute_imbalance_ratio(self):\n",
        "        class_distribution = self.get_class_distribution()\n",
        "        max_count = max(class_distribution.values())\n",
        "        min_count = min(class_distribution.values())\n",
        "        return max_count / min_count\n",
        "\n",
        "    def get_random_batch(self, batch_size=32):\n",
        "        indices = np.random.choice(self.indices, batch_size, replace=False)\n",
        "        return [self.original_dataset[idx] for idx in indices]\n",
        "\n",
        "    def extract_features_and_labels(dataset):\n",
        "        features = []\n",
        "        labels = []\n",
        "\n",
        "        for img, label in dataset:\n",
        "        # Flatten the image and convert to numpy array\n",
        "          flattened_img = torch.flatten(img).numpy()\n",
        "          features.append(flattened_img)\n",
        "          labels.append(label)\n",
        "\n",
        "        return np.array(features), np.array(labels)\n",
        "\n",
        "    def visualize_with_tsne(features, labels, class_names):\n",
        "        tsne = TSNE(n_components=2, random_state=123)\n",
        "        tsne_results = tsne.fit_transform(features)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            indices = labels == i\n",
        "            plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=class_name)\n",
        "        plt.legend()\n",
        "        plt.title('t-SNE visualization of the dataset')\n",
        "        plt.xlabel('t-SNE feature 1')\n",
        "        plt.ylabel('t-SNE feature 2')\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_with_pca(features, labels, class_names):\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_results = pca.fit_transform(features)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, class_name in enumerate(class_names):\n",
        "          indices = labels == i\n",
        "          plt.scatter(pca_results[indices, 0], pca_results[indices, 1], label=class_name)\n",
        "        plt.legend()\n",
        "        plt.title('PCA visualization of the dataset')\n",
        "        plt.xlabel('PCA feature 1')\n",
        "        plt.ylabel('PCA feature 2')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImbalanceCIFAR10(ImbalanceCIFAR):\n",
        "    def __init__(self, root='./data', train=True, transform=None, imbalance_ratio=0.1):\n",
        "        super(ImbalanceCIFAR10, self).__init__(cifar_version=10, root=root, train=train, transform=transform, imbalance_ratio=imbalance_ratio)\n",
        "\n",
        "\n",
        "class ImbalanceCIFAR100(ImbalanceCIFAR):\n",
        "    def __init__(self, root='./data', train=True, transform=None, imbalance_ratio=0.1):\n",
        "        super(ImbalanceCIFAR100, self).__init__(cifar_version=100, root=root, train=train, transform=transform, imbalance_ratio=imbalance_ratio)\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Initialize the datasets\n",
        "cifar10_lt = ImbalanceCIFAR10(train=True, transform=transform, imbalance_ratio=0.1)\n",
        "cifar100_lt = ImbalanceCIFAR100(train=True, transform=transform, imbalance_ratio=0.02)\n",
        "cifar10_test = ImbalanceCIFAR10(train=False, transform=test_transform, imbalance_ratio=1)  # imbalance_ratio=1 to keep original distribution\n",
        "cifar100_test = ImbalanceCIFAR100(train=False, transform=test_transform, imbalance_ratio=1)  # imbalance_ratio=1 to keep original distribution\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isjXgwItnBFL",
        "outputId": "3b08ae4d-5a9b-4f16-8f0e-716f09fe406f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_distribution = cifar100_lt.get_class_distribution()  # Use the appropriate dataset instance\n",
        "class_counts = list(class_distribution.values())"
      ],
      "metadata": {
        "id": "mDdLrDpnb710"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_freq = [1.0 / count for count in class_counts]\n",
        "sum_inv_freq = sum(inverse_freq)\n",
        "normalized_weights = [x / sum_inv_freq for x in inverse_freq]"
      ],
      "metadata": {
        "id": "ZB2vfY1rcIMq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_tensor = torch.tensor(normalized_weights).to(device)"
      ],
      "metadata": {
        "id": "A1mKeEXtcL8D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MixupLTDataloader():\n",
        "  def __init__(self, batch_size, dataset, alpha=0.2):\n",
        "    self.n_samples = len(dataset)\n",
        "    self.n_classes = dataset.num_classes\n",
        "    self.batch_size = batch_size\n",
        "    self.alpha = alpha\n",
        "    self.n_iterations = math.ceil(self.n_samples / batch_size)\n",
        "    self.labels = np.array(dataset.targets)\n",
        "    self.dataset = dataset\n",
        "\n",
        "  def random_class(self):\n",
        "    return np.random.randint(self.n_classes)\n",
        "\n",
        "  def random_sample(self, random_class):\n",
        "    # Randomly get index from the potential indices in the class\n",
        "    indices = np.where(self.labels == random_class)[0] # this is a numpy array\n",
        "    rand_idx = np.random.randint(len(indices)+1)\n",
        "    rand_sample, rand_label = self.dataset.__getitem__(rand_idx)\n",
        "    return rand_sample , rand_label\n",
        "\n",
        "  def generate_sample(self):\n",
        "    rand_class = self.random_class()\n",
        "    return self.random_sample(rand_class)\n",
        "\n",
        "  def generate_mixup(self):\n",
        "    # Define two samplers\n",
        "    probability = torch.zeros(self.num_classes)\n",
        "\n",
        "    x1, y1 = self.generate_sample()\n",
        "    x2, y2 = self.generate_sample()\n",
        "\n",
        "    x_hat = self.alpha * x1 + (1 - self.alpha) * x2\n",
        "    y_hat = self.alpha * y1 + (1 - self.alpha) * y2\n",
        "\n",
        "    c1 = math.floor(y_hat)\n",
        "    decimal = y_hat - c1\n",
        "    if c1 != self.num_classes -1:\n",
        "      c2 = c1 + 1\n",
        "    probability[c1] = 1 - decimal\n",
        "    probability[c2] = decimal\n",
        "\n",
        "    # y_hat = round(y_hat)\n",
        "    # y_hat = torch.LongTensor([y_hat])\n",
        "    # return x_hat, y_hat\n",
        "    return x_hat , torch.FloatTensor(probability)\n",
        "\n",
        "  def generate_batch(self):\n",
        "    data = []\n",
        "    labels = []\n",
        "    while len(data) < self.batch_size:\n",
        "      x, y = self.generate_mixup()\n",
        "      data.append(x)\n",
        "      labels.append(y)\n",
        "\n",
        "    return torch.stack(data), torch.stack(labels)"
      ],
      "metadata": {
        "id": "MUfKbbd5XPdf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixup = MixupLTDataloader(dataset=cifar100_lt,batch_size=4)"
      ],
      "metadata": {
        "id": "u_KBQXj8YVXy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ResNet in PyTorch.\n",
        "ImageNet-Style ResNet\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "Adapted from: https://github.com/bearpaw/pytorch-classification\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.is_last = is_last\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        preact = out\n",
        "        out = F.relu(out)\n",
        "        if self.is_last:\n",
        "            return out, preact\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves\n",
        "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
        "        # https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = strides[i]\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, layer=100):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "\n",
        "\n",
        "model_dict = {\n",
        "    'resnet18': [resnet18, 512],\n",
        "    'resnet34': [resnet34, 512],\n",
        "    'resnet50': [resnet50, 2048],\n",
        "    'resnet101': [resnet101, 2048],\n",
        "}\n",
        "\n",
        "\n",
        "class LinearBatchNorm(nn.Module):\n",
        "    \"\"\"Implements BatchNorm1d by BatchNorm2d, for SyncBN purpose\"\"\"\n",
        "    def __init__(self, dim, affine=True):\n",
        "        super(LinearBatchNorm, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.bn = nn.BatchNorm2d(dim, affine=affine)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.dim, 1, 1)\n",
        "        x = self.bn(x)\n",
        "        x = x.view(-1, self.dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SupConResNet(nn.Module):\n",
        "    \"\"\"backbone + projection head\"\"\"\n",
        "    def __init__(self, name='resnet50', head='mlp', feat_dim=128, num_classes = 10):\n",
        "        super(SupConResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        self.num_classes = num_classes\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "        if head == 'linear':\n",
        "            self.head = nn.Linear(dim_in, feat_dim)\n",
        "        elif head == 'mlp':\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(dim_in, dim_in),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(dim_in, feat_dim)\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                'head not supported: {}'.format(head))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feat = self.encoder(x)\n",
        "        feat = F.normalize(self.head(feat), dim=1)\n",
        "        bsz = int(feat.shape[0]/2)\n",
        "        f1, f2 = torch.split(feat, [bsz, bsz], dim=0)\n",
        "        x = self.fc(f1)\n",
        "        return feat, x\n",
        "\n",
        "\n",
        "#Cross entropy model\n",
        "class SupCEResNet(nn.Module):\n",
        "    \"\"\"encoder + classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(SupCEResNet, self).__init__()\n",
        "        model_fun, dim_in = model_dict[name]\n",
        "        self.encoder = model_fun()\n",
        "        self.fc = nn.Linear(dim_in, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.encoder(x))\n",
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"Linear classifier\"\"\"\n",
        "    def __init__(self, name='resnet50', num_classes=10):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        _, feat_dim = model_dict[name]\n",
        "        self.fc = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)"
      ],
      "metadata": {
        "id": "LjXHNamnn68E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoCropTransform:\n",
        "    \"\"\"Create two crops of the same image\"\"\"\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.transform(x), self.transform(x)]\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, epoch):\n",
        "    lr = args.learning_rate\n",
        "    if args.cosine:\n",
        "        eta_min = lr * (args.lr_decay_rate ** 3)\n",
        "        lr = eta_min + (lr - eta_min) * (\n",
        "                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
        "    else:\n",
        "        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
        "        if steps > 0:\n",
        "            lr = lr * (args.lr_decay_rate ** steps)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n",
        "    if args.warm and epoch <= args.warm_epochs:\n",
        "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
        "            (args.warm_epochs * total_batches)\n",
        "        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def set_optimizer(opt, model):\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=opt.learning_rate,\n",
        "                          momentum=opt.momentum,\n",
        "                          weight_decay=opt.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, opt, epoch, save_file):\n",
        "    print('==> Saving...')\n",
        "    state = {\n",
        "        'opt': opt,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(state, save_file)\n",
        "    del state"
      ],
      "metadata": {
        "id": "HqcO58opoHXr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "LeBu_09gpYrz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Sequence\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
        "\n",
        "    It is essentially an enhancement to cross entropy loss and is\n",
        "    useful for classification tasks when there is a large class imbalance.\n",
        "    x is expected to contain raw, unnormalized scores for each class.\n",
        "    y is expected to contain class labels.\n",
        "\n",
        "    Shape:\n",
        "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
        "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 alpha: Optional[Tensor] = None,\n",
        "                 gamma: float = 0.,\n",
        "                 reduction: str = 'mean',\n",
        "                 ignore_index: int = -100):\n",
        "        \"\"\"Constructor.\n",
        "\n",
        "        Args:\n",
        "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
        "            gamma (float, optional): A constant, as described in the paper.\n",
        "                Defaults to 0.\n",
        "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
        "                Defaults to 'mean'.\n",
        "            ignore_index (int, optional): class label to ignore.\n",
        "                Defaults to -100.\n",
        "        \"\"\"\n",
        "        if reduction not in ('mean', 'sum', 'none'):\n",
        "            raise ValueError(\n",
        "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
        "\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "        self.nll_loss = nn.NLLLoss(\n",
        "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
        "\n",
        "    def __repr__(self):\n",
        "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
        "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
        "        arg_strs = [f'{k}={v!r}' for k, v in zip(arg_keys, arg_vals)]\n",
        "        arg_str = ', '.join(arg_strs)\n",
        "        return f'{type(self).__name__}({arg_str})'\n",
        "\n",
        "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
        "        if x.ndim > 2:\n",
        "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
        "            c = x.shape[1]\n",
        "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
        "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
        "            y = y.view(-1)\n",
        "\n",
        "        unignored_mask = y != self.ignore_index\n",
        "        y = y[unignored_mask]\n",
        "        if len(y) == 0:\n",
        "            return torch.tensor(0.)\n",
        "        x = x[unignored_mask]\n",
        "\n",
        "        # compute weighted cross entropy term: -alpha * log(pt)\n",
        "        # (alpha is already part of self.nll_loss)\n",
        "        log_p = F.log_softmax(x, dim=-1)\n",
        "        ce = self.nll_loss(log_p, y)\n",
        "\n",
        "        # get true class column from each row\n",
        "        all_rows = torch.arange(len(x))\n",
        "        log_pt = log_p[all_rows, y]\n",
        "\n",
        "        # compute focal term: (1 - pt)^gamma\n",
        "        pt = log_pt.exp()\n",
        "        focal_term = (1 - pt)**self.gamma\n",
        "\n",
        "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
        "        loss = focal_term * ce\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def focal_loss(alpha: Optional[Sequence] = None,\n",
        "               gamma: float = 0.,\n",
        "               reduction: str = 'mean',\n",
        "               ignore_index: int = -100,\n",
        "               device='cpu',\n",
        "               dtype=torch.float32) -> FocalLoss:\n",
        "    \"\"\"Factory function for FocalLoss.\n",
        "\n",
        "    Args:\n",
        "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
        "            to a Tensor if not None. Defaults to None.\n",
        "        gamma (float, optional): A constant, as described in the paper.\n",
        "            Defaults to 0.\n",
        "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
        "            Defaults to 'mean'.\n",
        "        ignore_index (int, optional): class label to ignore.\n",
        "            Defaults to -100.\n",
        "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
        "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
        "            Defaults to torch.float32.\n",
        "\n",
        "    Returns:\n",
        "        A FocalLoss object\n",
        "    \"\"\"\n",
        "    if alpha is not None:\n",
        "        if not isinstance(alpha, Tensor):\n",
        "            alpha = torch.tensor(alpha)\n",
        "        alpha = alpha.to(device=device, dtype=dtype)\n",
        "\n",
        "    fl = FocalLoss(\n",
        "        alpha=alpha,\n",
        "        gamma=gamma,\n",
        "        reduction=reduction,\n",
        "        ignore_index=ignore_index)\n",
        "    return fl"
      ],
      "metadata": {
        "id": "6N3-bggNvK1F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "f7qD2-ozi-3j",
        "outputId": "75fb0a69-1f91-4ccf-ebe6-243692a2f853"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader_cifar100_lt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bcd58462db1d>\u001b[0m in \u001b[0;36m<cell line: 328>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-bcd58462db1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_cifar100_lt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, total time {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader_cifar100_lt' is not defined"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "\n",
        "import tensorboard_logger as tb_logger\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def parse_option(batch_size=32, epochs=10, print_freq=10, save_freq=9, num_workers=16,\n",
        "                 learning_rate=0.05, lr_decay_epochs='700,800,900', lr_decay_rate=0.1,\n",
        "                 weight_decay=1e-4, momentum=0.9, model='resnet50', dataset='cifar100',\n",
        "                 mean=None, std=None, data_folder=None, size=32, method='SupCon',\n",
        "                 temp=0.07, cosine=False, syncBN=False, warm=False, trial='0'):\n",
        "    class Option(object):\n",
        "        pass\n",
        "\n",
        "    opt = Option()\n",
        "\n",
        "    # Set values\n",
        "    opt.batch_size = batch_size\n",
        "    opt.epochs = epochs\n",
        "    opt.print_freq = print_freq\n",
        "    opt.save_freq = save_freq\n",
        "    opt.num_workers = num_workers\n",
        "    opt.learning_rate = learning_rate\n",
        "    opt.lr_decay_epochs = lr_decay_epochs\n",
        "    opt.lr_decay_rate = lr_decay_rate\n",
        "    opt.weight_decay = weight_decay\n",
        "    opt.momentum = momentum\n",
        "    opt.model = model\n",
        "    opt.dataset = dataset\n",
        "    opt.mean = mean\n",
        "    opt.std = std\n",
        "    opt.data_folder = data_folder\n",
        "    opt.size = size\n",
        "    opt.method = method\n",
        "    opt.temp = temp\n",
        "    opt.cosine = cosine\n",
        "    opt.syncBN = syncBN\n",
        "    opt.warm = warm\n",
        "    opt.trial = trial\n",
        "\n",
        "\n",
        "\n",
        "    # Additional processing as in the original function\n",
        "    if opt.dataset == 'path':\n",
        "        assert opt.data_folder is not None and opt.mean is not None and opt.std is not None\n",
        "\n",
        "    if opt.data_folder is None:\n",
        "        opt.data_folder = './datasets/'\n",
        "\n",
        "    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n",
        "    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n",
        "\n",
        "    iterations = opt.lr_decay_epochs.split(',')\n",
        "    opt.lr_decay_epochs = [int(it) for it in iterations]\n",
        "\n",
        "    opt.model_name = 'SupCE_{}_{}_lr_{}_decay_{}_bsz_{}_trial_{}'.\\\n",
        "        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
        "               opt.batch_size, opt.trial)\n",
        "\n",
        "    if opt.cosine:\n",
        "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
        "\n",
        "    # warm-up for large-batch training,\n",
        "    if opt.batch_size > 256:\n",
        "        opt.warm = True\n",
        "    if opt.warm:\n",
        "        opt.model_name = '{}_warm'.format(opt.model_name)\n",
        "        opt.warmup_from = 0.01\n",
        "        opt.warm_epochs = 10\n",
        "        if opt.cosine:\n",
        "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
        "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
        "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
        "        else:\n",
        "            opt.warmup_to = opt.learning_rate\n",
        "\n",
        "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
        "    if not os.path.isdir(opt.tb_folder):\n",
        "        os.makedirs(opt.tb_folder)\n",
        "\n",
        "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
        "    if not os.path.isdir(opt.save_folder):\n",
        "        os.makedirs(opt.save_folder)\n",
        "\n",
        "    if opt.dataset == 'cifar10':\n",
        "        opt.n_cls = 10\n",
        "    elif opt.dataset == 'cifar100':\n",
        "        opt.n_cls = 100\n",
        "    else:\n",
        "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
        "\n",
        "    return opt\n",
        "\n",
        "\n",
        "def set_loader(opt):\n",
        "    # construct data loader\n",
        "    if opt.dataset == 'cifar10':\n",
        "        mean = (0.4914, 0.4822, 0.4465)\n",
        "        std = (0.2023, 0.1994, 0.2010)\n",
        "    elif opt.dataset == 'cifar100':\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "    else:\n",
        "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    if opt.dataset == 'cifar10':\n",
        "        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
        "                                         transform=train_transform,\n",
        "                                         download=True)\n",
        "        val_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
        "                                       train=False,\n",
        "                                       transform=val_transform)\n",
        "    elif opt.dataset == 'cifar100':\n",
        "        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
        "                                          transform=train_transform,\n",
        "                                          download=True)\n",
        "        val_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
        "                                        train=False,\n",
        "                                        transform=val_transform)\n",
        "    else:\n",
        "        raise ValueError(opt.dataset)\n",
        "\n",
        "    train_sampler = None\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n",
        "        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=256, shuffle=False,\n",
        "        num_workers=8, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def set_model(opt,use_CE= True):\n",
        "    model = SupCEResNet(name=opt.model, num_classes=opt.n_cls)\n",
        "    if use_CE:\n",
        "      criterion = torch.nn.CrossEntropyLoss()\n",
        "    else:\n",
        "       criterion = focal_loss(alpha=weights_tensor, gamma=2.0, reduction='mean')\n",
        "\n",
        "    # enable synchronized Batch Normalization\n",
        "    if opt.syncBN:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    return model, criterion\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, opt):\n",
        "    \"\"\"one epoch training\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    iterations = math.ceil(mixup.n_samples / mixup.batch_size)\n",
        "\n",
        "    for idx in range(iterations):\n",
        "        images , labels = mixup.generate_batch()\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.to(device)\n",
        "        lables = labels.to(device)\n",
        "\n",
        "        # images = images.cuda(non_blocking=True)\n",
        "        # labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.shape[0]\n",
        "\n",
        "        # warm-up learning rate\n",
        "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
        "\n",
        "        # compute loss\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels.squeeze())\n",
        "\n",
        "        # update metric\n",
        "        losses.update(loss.item(), bsz)\n",
        "        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "        top1.update(acc1[0], bsz)\n",
        "\n",
        "        # SGD\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print info\n",
        "        if (idx + 1) % opt.print_freq == 0:\n",
        "            print('Train: [{0}][{1}/{2}]\\t'\n",
        "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
        "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, opt):\n",
        "    \"\"\"validation\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images = images.float().cuda()\n",
        "            labels = labels.cuda()\n",
        "            bsz = labels.shape[0]\n",
        "\n",
        "            # forward\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # update metric\n",
        "            losses.update(loss.item(), bsz)\n",
        "            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
        "            top1.update(acc1[0], bsz)\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if idx % opt.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                       idx, len(val_loader), batch_time=batch_time,\n",
        "                       loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "def main():\n",
        "    best_acc = 0\n",
        "    opt = parse_option()\n",
        "\n",
        "    # build data loader\n",
        "    # train_loader, val_loader = train_loader_cifar100_lt\n",
        "\n",
        "    # build model and criterion\n",
        "    model, criterion = set_model(opt,False)\n",
        "\n",
        "    # build optimizer\n",
        "    optimizer = set_optimizer(opt, model)\n",
        "\n",
        "    # tensorboard\n",
        "    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
        "\n",
        "    # training routine\n",
        "    for epoch in range(1, opt.epochs + 1):\n",
        "        adjust_learning_rate(opt, optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        time1 = time.time()\n",
        "        loss, train_acc = train(train_loader_cifar100_lt, model, criterion, optimizer, epoch, opt)\n",
        "        time2 = time.time()\n",
        "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
        "\n",
        "        # tensorboard logger\n",
        "        logger.log_value('train_loss', loss, epoch)\n",
        "        logger.log_value('train_acc', train_acc, epoch)\n",
        "        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "        # evaluation\n",
        "        loss, val_acc = validate(val_loader_cifar100_lt, model, criterion, opt)\n",
        "        logger.log_value('val_loss', loss, epoch)\n",
        "        logger.log_value('val_acc', val_acc, epoch)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "\n",
        "        if epoch % opt.save_freq == 0:\n",
        "            save_file = os.path.join(\n",
        "                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
        "            save_model(model, optimizer, opt, epoch, save_file)\n",
        "\n",
        "    # save the last model\n",
        "    save_file = os.path.join(\n",
        "        opt.save_folder, 'last.pth')\n",
        "    save_model(model, optimizer, opt, opt.epochs, save_file)\n",
        "\n",
        "    print('best accuracy: {:.2f}'.format(best_acc))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nr-y4rrLwq5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}